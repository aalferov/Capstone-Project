---
title: "Data Science Capstone Milestone Report"
author: "Andrey Alferov"
date: "26 July 2015"
output: html_document
---

##Introduction
The purpose of this report is just to display the results of getting and cleaning data (text files provided by SwiftKey) and initial exploratory analysis. The report also contains the description of further plans to build the prediction model and the app.

##Getting Data
Load all required libraries.
```{r warning=FALSE, message=FALSE}
library(tm)
library(ggplot2)
library(R.utils)
library(stringi)
library(RWeka)
library(knitr)
library(qdapRegex)
```

Download and unzip source file.
```{r}
setwd("D:/Coursera/Data Science/Capstone Project") #Path should be replaced by your own when reproducing the analysis

url  <- "http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
fileName <- "coursera-swiftkey.zip"
if (!file.exists(fileName)){
  download.file(url = url, destfile = fileName)
  filelist=c("final/en_US/en_US.twitter.txt", "final/en_US/en_US.news.txt","final/en_US/en_US.blogs.txt") 
  unzip(fileName, files = filelist, exdir = "en_US", overwrite = TRUE, junkpaths = TRUE)  
}
```

Load text files in memory.
```{r warning=FALSE}
conBlogs <- file("final/en_US/en_US.blogs.txt", 'r')
conNews <- file("final/en_US/en_US.news.txt", 'r')
conTwitter <- file("final/en_US/en_US.twitter.txt", 'r')
textBlogs <- readLines(conBlogs)
textNews <- readLines(conNews)
textTwitter <- readLines(conTwitter)
close(conBlogs)
close(conNews)
close(conTwitter)
```

Calculate basic statistics of the text files.
```{r warning=FALSE, cache=TRUE}
# get size of files
fsBlogs <- file.info("final/en_US/en_US.blogs.txt")$size
fsNews <- file.info("final/en_US/en_US.news.txt")$size
fsTwitter <- file.info("final/en_US/en_US.twitter.txt")$size
fileSize <- c(fsBlogs, fsNews, fsTwitter)

# calculate line counts
lcBlogs <- length(textBlogs)
lcNews <- length(textNews)
lcTwitter <- length(textTwitter)
lineCount <- c(lcBlogs, lcNews, lcTwitter)

# calculate word counts
wcBlogs <- sum(stri_count_words(textBlogs))
wcNews <- sum(stri_count_words(textNews))
wcTwitter <- sum(stri_count_words(textTwitter))
wordCount <- c(wcBlogs, wcNews, wcTwitter)

info <- data.frame(row.names=c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"), FileSize = fileSize, LineCount = lineCount, WordCount = wordCount)
kable(info, format="markdown")
```

##Cleaning and Preprocessing Data
As a first step lets extract 1000 text lines randomly and use them as a sample distribution to perform the following transformations and analysis in order to reduce processing time.
```{r warning=FALSE, cache=TRUE}
textBlogs <- sample(textBlogs, 1000)
textNews <- sample(textNews, 1000)
textTwitter <- sample(textTwitter, 1000)
```

As a second step lets remove all weird characters from the sampled texts and combine them into a single text object.
```{r warning=FALSE, cache=TRUE}
# convert to "ASCII" encoding to remove weird characters
textBlogs <- iconv(textBlogs, to="ASCII", sub="")
textNews <- iconv(textNews, to="ASCII", sub="")
textTwitter <- iconv(textTwitter, to="ASCII", sub="")

# combine text arrays into a single one 
textUnited <- paste(textBlogs, textNews, textTwitter)
```

As a third step lets do the following transformations to clean the united text array:
*Remove twitter retweets 
*Remove @people tags, 
*Remove URLs, 
*Remove emoticons and hash tags
```{r warning=FALSE, cache=TRUE}
textUnited = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", textUnited)
textUnited = gsub("@\\w+", "", textUnited)
textUnited = gsub("http\\w+", "", textUnited)
textUnited <- rm_emoticon(textUnited)
textUnited <- rm_hash(textUnited)
textUnited <- rm_url(textUnited)
```

As a fourth step lets convert sampled text data into the corpus of structured text, suitable for permofring statistical analysis.
```{r warning=FALSE, cache=TRUE}
textCorpus <- Corpus(VectorSource(textUnited))
```

As a fifth step lets do the following transformations to clean the corpus:
*Convert to lower case
*Remove punctuation
*Remove numbers
*Strip white spaces
```{r warning=FALSE, cache=TRUE}
textCorpus <- tm_map(textCorpus, stripWhitespace)
textCorpus <- tm_map(textCorpus, removeNumbers)
textCorpus <- tm_map(textCorpus, removePunctuation)
textCorpus <- tm_map(textCorpus, PlainTextDocument)
textCorpus <- tm_map(textCorpus, content_transformer(tolower))
```

As a sixth step lets filter out profanity words from the corpus as we don't want to predict them.
```{r warning=FALSE, cache=TRUE}
# download the list of profanity words
url <- "https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
fileName <- "profanity.txt"

if (!file.exists(fileName))
    download.file(url, destfile=fileName)

profanityWords <- readLines(fileName)

# filter them out of the corpus
textCorpus <- tm_map(textCorpus, removeWords, profanityWords)
```

## Exploratory Analysis
In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus.

Only unigrams, bigrams and trigrams will be analized in order to make analysis not too complex and in respect of resources limits.

```{r warning=FALSE, cache=TRUE}
unigramTokenizer <- function (text)
{
  NGramTokenizer(text, Weka_control(min = 1, max = 1))
}

bigramTokenizer <- function (text)
{
  NGramTokenizer(text, Weka_control(min = 2, max = 2))
}

trigramTokenizer <- function (text)
{
  NGramTokenizer(text, Weka_control(min = 3, max = 3))
}

#Build document-term matrices based on unigram, bigram and trigram tokenizer, e.g. matricies containing frequencies of one, two, free grams appearance in the text corpus
dtmUnigram <- DocumentTermMatrix(textCorpus, control=list(tokenize=unigramTokenizer))
dtmBigram <- DocumentTermMatrix(textCorpus, control=list(tokenize=bigramTokenizer))
dtmTrigram <- DocumentTermMatrix(textCorpus, control=list(tokenize=trigramTokenizer))
```

Convert the document-term matrices to normal matrices, sum up all the terms (words) and sort them.

```{r warning=FALSE, cache=TRUE}
unigramFrequency  <- sort(colSums(as.matrix(dtmUnigram)), decreasing=TRUE)
dfUnigram <- data.frame(word=names(unigramFrequency), freq=unigramFrequency, stringsAsFactors = FALSE)

bigramFrequency <- sort(colSums(as.matrix(dtmBigram)), decreasing=TRUE)
dfBigram <- data.frame(word=names(bigramFrequency), freq=bigramFrequency, stringsAsFactors = FALSE)

trigramFrequency <- sort(colSums(as.matrix(dtmTrigram)), decreasing=TRUE)
dfTrigram <- data.frame(word=names(trigramFrequency), freq=trigramFrequency, stringsAsFactors = FALSE)
```

Show the frequinces distribution for top 20 most frequent unigrams, bigrams and trigrams.

```{r warning=FALSE}
barplot(unigramFrequency[1:20], ylab='frequency', main='top 20 most frequent unigrams', names.arg=names(unigramFrequency)[1:20],        
        col="red", las=2, cex.names=.7)

barplot(bigramFrequency[1:20], ylab='frequency', main='top 20 most frequent bigrams', names.arg=names(bigramFrequency)[1:20],        
        col="green", las=2, cex.names=.7)

barplot(trigramFrequency[1:20], ylab='frequency', main='top 20 most frequent trigrams', names.arg=names(trigramFrequency)[1:20],        
        col="blue", las=2, cex.names=.7)
```

## Further steps
In order to imporove the accuracy, I’m going to do following works: In the next couple of weeks, I will build an algorithm to predict the next work, based on the n-gram models. The algorithm involves building a lookup table for each n-gram, and use it to predict the next word in a sentence. The algorithm will search in the 4-gram model, then the 3-gram, and then in the 2-gram. If there is no match, then we need to come up with the best guess. The algorithm will assume that the whole English language is covered in the corpa (very optimistic). The algorithm has to be fast in searching and predicting a word. A shiny app will be built to demonstrate the algorithm to predict a word after an input phrase.
